<?xml version="1.0" encoding="UTF-8"?>

<alvisnlp-plan id="export">


  <!--
  ///////////////////////////////////////////////////////////////////////////////////////
  //
  //  FCU
  //
  ///////////////////////////////////////////////////////////////////////////////////////
  -->


  <!-- save exctracted terms -->
  <export class="TabularExport">
    <outDir>output</outDir>
    <corpusFile>export.csv</corpusFile>
    <lines>documents.sections.(layer:fcu | layer:stages)</lines>
    <headers>"bsv","type","context","word","prefLabel","context","location","features"</headers>
    <columns separator=";">
      <!-- short document id  -->
      str:replace(str:basename(section.document.@id), ".html", "");
      <!-- type of document  -->
      section.@name;
      <!-- context before  -->
      str:normalizeSpace(str:sub(section.contents, m:max(0, start - 20), start));
      <!-- extracted term  -->
      @form;
      @skos-prefLabel;
      <!-- context after  -->
      str:normalizeSpace(str:sub(section.contents, end, m:min(end + 20, str:len(section.contents))));
      <!-- location  -->
      start ^ "-" ^ end;
      <!-- features  -->
      str:join:', '(nav:features, @key ^ "=" ^ @value)
    </columns>
    <trueCSV/>
  </export>

  <!--
  ///////////////////////////////////////////////////////////////////////////////////////
  //
  //  Conjunctions
  //
  ///////////////////////////////////////////////////////////////////////////////////////
  -->

  <conj_export class="TabularExport">
    <outDir>output</outDir>
    <corpusFile>conjunctions.csv</corpusFile>
    <lines>documents.sections.layer:conjunctions[inside:fcu]</lines>
    <headers>"bsv","context", "word", "context", "location","features"</headers>
    <columns separator=";">
      <!-- short document id  -->
      str:replace(str:basename(section.document.@id), ".html", "");
      <!-- context before  -->
      str:normalizeSpace(str:sub(section.contents, m:max(0, start - 20), start));
      <!-- extracted term  -->
      @form;
      <!-- context after  -->
      str:normalizeSpace(str:sub(section.contents, end, m:min(end + 20, str:len(section.contents))));
      <!-- location  -->
      start ^ "-" ^ end;
      <!-- features  -->
      str:join:', '(nav:features, @key ^ "=" ^ @value)
    </columns>
    <trueCSV/>
  </conj_export>


  <!--
  ///////////////////////////////////////////////////////////////////////////////////////
  //
  //  Stages
  //
  ///////////////////////////////////////////////////////////////////////////////////////
  -->


  <bbch_export class="TabularExport">
    <outDir>output</outDir>
    <corpusFile>bbch.csv</corpusFile>
    <lines>documents.sections.layer:bbch</lines>
    <headers>"bsv","context", "word", "context", "location","features"</headers>
    <columns separator=";">
      <!-- short document id  -->
      str:replace(str:basename(section.document.@id), ".html", "");
      <!-- context before  -->
      str:normalizeSpace(str:sub(section.contents, m:max(0, start - 20), start));
      <!-- extracted term  -->
      @form;
      <!-- context after  -->
      str:normalizeSpace(str:sub(section.contents, end, m:min(end + 20, str:len(section.contents))));
      <!-- location  -->
      start ^ "-" ^ end;
      <!-- features  -->
      str:join:', '(nav:features, @key ^ "=" ^ @value)

    </columns>
    <trueCSV/>
  </bbch_export>


  <vocab class="AggregateValues">
    <entries>documents.sections.layer:bbch</entries>
    <key>@form</key>
    <outFile>output/bbch.txt</outFile>
  </vocab>


  <!--
  ///////////////////////////////////////////////////////////////////////////////////////
  //
  //  Save tokenized words
  //
  ///////////////////////////////////////////////////////////////////////////////////////
  -->


  <!-- all tokenized words in corpus -->
  <vocabulary class="AggregateValues">
    <entries>documents.sections.layer:words</entries>
    <key>@lemma</key>
    <outFile>output/words.txt</outFile>
  </vocabulary>




  <!--
  ///////////////////////////////////////////////////////////////////////////////////////
  //
  //  Save scores
  //
  ///////////////////////////////////////////////////////////////////////////////////////
  -->



  <tf_idf href="modules/scores.plan">
    <outFile>output/tfidf.csv</outFile>
    <keywords>sections.layer:fcu</keywords>
    <scoreFunction>tfidf</scoreFunction>
  </tf_idf>

  <bm25 href="modules/scores.plan">
    <outFile>output/bm25.csv</outFile>
    <keywords>sections.layer:fcu</keywords>
    <scoreFunction type ="bm25" k1="1.2" b="0.75"/>
  </bm25>


  <!-- TODO II. topic modelling  -->

</alvisnlp-plan>



<!--
before:words{-3}.@form;
before:words{-2}.@form;
before:words{-1}.@form;
after:words{0}.@form;
after:words{1}.@form;
after:words{2}.@form;

-->

<!-- features  ;
str:join:' '(inside:words, @lemma)
-->
